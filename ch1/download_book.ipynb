{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download-png1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 읽어 들이기 --- (※1)\n",
    "import urllib.request\n",
    "# urllib 패키지 내부에 있는 request 모듈\n",
    "\n",
    "# URL과 저장 경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"test.png\"\n",
    "\n",
    "# 다운로드 --- (※2)\n",
    "urllib.request.urlretrieve(url, savename) \n",
    "# 첫 번째는 URL을, 두 번째는 매개변수에 저장할 파일의 경로\n",
    "print(\"저장되었습니다...!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download-png2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# URL과 저장 경로 지정하기\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"test.png\"\n",
    "\n",
    "# 다운로드 -- (*1)\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "# request.urlretrieve() = 파일에 곧바로 저장 \n",
    "# request.urlopen() = 데이터를 파이썬 메모리 위에 올린다. \n",
    "# read() 메서드로 데이터를 읽어 들입니다.\n",
    "\n",
    "# 파일로 저장하기 -- (*2)\n",
    "with open(savename, mode=\"wb\") as f:\n",
    "    # wb = \"w\" 쓰기 모드, \"b\" 바이너리 모드\n",
    "    f.write(mem)\n",
    "    print(\"저장되었습니다...!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download-ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IP 확인 API로 접근해서 결과 출력하기\n",
    "# 모듈 읽어 들이기 --(*1)\n",
    "import urllib.request\n",
    "\n",
    "# 데이터 읽어 들이기 --(*2)\n",
    "url = \"http://api.aoikujira.com/ip/ini\"\n",
    "#ftp상의 리소스를 추출할 때는 http:// => tfp://\n",
    "res = urllib.request.urlopen(url)\n",
    "data = res.read()\n",
    "\n",
    "# 바이너리를 문자열로 변환하기 --(*3)\n",
    "text = data.decode(\"utf-8\")\n",
    "# 바이너리를 문자열로 변환\n",
    "print(text)\n",
    "# print = 표준 출력에 데이터를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download-forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "API = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "# 매개변수를 URL 인코딩합니다. -- (*1)\n",
    "values = {\n",
    "    'stnId': '108'\n",
    "}\n",
    "params = urllib.parse.urlencode(values)\n",
    "\n",
    "# 요청 전용 URL을 생성합니다. -- (*2)\n",
    "url = API + \"?\" + params\n",
    "print(\"url=\",url)\n",
    "\n",
    "#다운로드합니다 -- (*3)\n",
    "data = urllib.request.urlopen(url).read()\n",
    "text = data.decode(\"utf-8\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### download-forecast-argv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=-f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !/usr/bin/env python3\n",
    "\n",
    "# 라이브러리를 읽어 들입니다. -- (*1)\n",
    "import sys\n",
    "import urllib.request as req\n",
    "import urllib.parse as parse\n",
    "\n",
    "# 명령줄 매개변수 추출 -- (*2)\n",
    "if len(sys.argv) <= 1:\n",
    "    print(\"USAGE: download-forecast-argv <Region Number>\")\n",
    "    sys.exit()\n",
    "regionNumber = sys.argv[1]\n",
    "\n",
    "# 매개변수를 URL 인코딩합니다. -- (*3)\n",
    "API = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "values = {\n",
    "    'stnId': regionNumber\n",
    "}\n",
    "params = parse.urlencode(values)\n",
    "url = API + \"?\" + params\n",
    "print(\"url =\", url)\n",
    "\n",
    "# 다운로드합니다. -- (*4)\n",
    "data = req.urlopen(url).read()\n",
    "text = data.decode(\"utf-8\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bs-test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 스크레이핑이란?\n",
      "p = 웹 페이지를 분석하는 것\n",
      "p = 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 분석하고 싶은 HTML -- (*1)\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "    <h1>스크레이핑이란?</h1>\n",
    "    <p>웹 페이지를 분석하는 것</p>\n",
    "    <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "#HTML 분석하기 -- (*2)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 원하는 부분 추출하기 -- (*3)\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "# 요소의 글자 출력하기 -- (*4)\n",
    "print(\"h1 = \" + h1.string)\n",
    "print(\"p = \" +p1.string)\n",
    "print(\"p = \" + p2.string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bs-test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title=스크레이핑이란?\n",
      "#body=웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 분석하고 싶은 HTML -- (*1)\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "  <h1 id=\"title\">스크레이핑이란?</h1>\n",
    "  <p id=\"body\">웹 페이지를 분석하는 것</p>\n",
    "  <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "#HTML 분석하기 -- (*2)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# find() 메서드로 원하는 부분 추출하기 -- (*3)\n",
    "title = soup.find(id=\"title\")\n",
    "body = soup.find(id=\"body\")\n",
    "\n",
    "#텍스트 부분 출력하기\n",
    "print(\"#title=\" + title.string)\n",
    "print(\"#body=\" + body.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bs-link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "    <ul>\n",
    "        <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "        <li><a href=\"http://www.daum.net\">daum</a><li>\n",
    "    </ul>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 -- (*1)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# find.all() 메서드로 추출하기 -- (*2)\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "# 링크 목록 출력하기 -- (*3)\n",
    "for a in links:\n",
    "    href = a.attrs['href']\n",
    "    text = a.string\n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bs-forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "○ (강수) 14일(금)~15일(토)은 중부지방(강원영동 제외), 16일(일)은 서울.경기도와 강원영서에 비가 오겠습니다.<br />○ (기온) 이번 예보기간의 낮 기온은 27~35도로 오늘(25~33도)보다 조금 높겠습니다.<br />          특히, 경상내륙을 중심으로 낮 기온이 35도 내외로 매우 덥겠고, 열대야가 나타나는 곳이 많겠습니다.<br />○ (주말전망) 15일(토) 중부지방(강원영동 제외), 16일(일) 서울.경기도와 강원영서에 비가 오겠고, 남부지방은 구름많겠습니다. <br />              아침 기온은 24~26도, 낮 기온은 27~35도 분포를 보이겠습니다.<br /><br />* 서울.경기도와 강원영서, 충청도에는 14일(금)부터 15일(토) 사이 정체전선 상에서 매우 많은 비가 올 가능성이 있으니, 앞으로 발표되는 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "# urlopen()으로 데이터 가져오기 --- (※1)\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup으로 분석하기 --- (※2)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 원하는 데이터 추출하기 --- (※3)\n",
    "title = soup.find(\"title\").string\n",
    "wf = soup.find(\"wf\").string\n",
    "print(title)\n",
    "print(wf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bs-select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 = 위키북스 도서\n",
      "li = 유니티 게임 이펙트 입문\n",
      "li = 스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "li = 모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 분석 대상 HTML -- (*1)\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <div id=\"meigen\">\n",
    "            <h1>위키북스 도서</h1>\n",
    "            <ul class=\"items\">\n",
    "                <li>유니티 게임 이펙트 입문</li>\n",
    "                <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "                <li>모던 웹사이트 디자인의 정석</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 -- (*2)\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 필요한 부분을 CSS 쿼리로 추출하기\n",
    "# 타이틀 부분 추출하기 -- (*3)\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string\n",
    "# soup.select_one(선택자) = CSS 선택자로 요소 하나를 추출합니다.\n",
    "print(\"h1 =\", h1)\n",
    "\n",
    "# 목록 부분 추출하기 -- (*4)\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\")\n",
    "# soup.select(선택자) = CSS 선택자로 요소 여러 개를 리스트로 추출합니다.\n",
    "for li in li_list:\n",
    "    print(\"li =\", li.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw = 1,184.50\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML 가져오기\n",
    "url = \"https://finance.naver.com/marketindex/?tabSel=exchange\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 원하는 데이터 추출하기 -- (*1)\n",
    "price = soup.select_one(\"div.head_info > span.value\").string\n",
    "print(\"usd/krw =\", price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sel-Stan Coveleski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1889년\n",
      "- 7월 13일\n",
      "- 1984년\n",
      "- 3월 20일\n",
      "- 미국\n",
      "- 프로 야구\n",
      "- 메이저 리그 베이스볼\n",
      "- 스핏볼\n",
      "- 투수\n",
      "- 아메리칸 리그\n",
      "- 필라델피아 애슬레틱스\n",
      "- 클리블랜드 인디언스\n",
      "- 워싱턴 세너터스\n",
      "- 뉴욕 양키스\n",
      "- 이닝\n",
      "- 완투\n",
      "- 완봉\n",
      "- 평균자책점\n",
      "- 야구 명예의 전당\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"https://ko.wikipedia.org/wiki/%EC%8A%A4%ED%83%A0_%EC%BD%94%EB%B2%A8%EB%A0%88%EC%8A%A4%ED%82%A4\"\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "a_list = soup.select(\"#mw-content-text > div.mw-parser-output > p:nth-child(3) > a\")\n",
    "# css 선택자 => \n",
    "## 선택자 기본 서식 =>\n",
    "# * = 모든 요소를 선택, <요소 이름> = 요소 이름을 기반으로 선택, .<클래스 이름> = 클래스 이름을 기반으로 선택,\n",
    "# #<id 이름> = id 속성을 기반으로 선택\n",
    "## 선택자들의 관계를 지정하는 서식 =>\n",
    "# <선택자>, <선택자> = 쉼표로 구분된 여러 개의 선택자 모두 선택, <선택자> <선택자> = 앞 선택자의 후손 중 뒤 선택자에 해당하는 모든 것을 선택\n",
    "# <선택자> > <선택자> = 앞 선택자의 자손 중 뒤 선택자에 해당하는 모든 것을 선택, <선택자> + <선택자> = 같은 계층에서 바로 뒤에 있는 요소 선택\n",
    "# <선택자1> - <선택자2> = 선택자1로부터 선택자2까지의 요소 모두 선택\n",
    "##  선택자 속성을 기반으로 지정하는 서식 =>\n",
    "# <요소>[<속성>] = 해당 속성을 가진 요소를 선택, <요소>[<속성>=<값>] = 해당 속성의 값이 지정한 값과 같은 요소를 선택\n",
    "# <요소>[<속성>~=<값>] = 해당 속성의 값이 지정한 값을 단어로 포함하고 있으면 선택, <요소>[<속성>^=<값>] = 해당 속성의 값이 지정한 값으로 시작하면 선택\n",
    "# <요소>[<속성>|=<값>] = 해당 속성의 값으로 시작하면 선택, <요소>[<속성>$=<값>] = 해당 속성의 값이 지정한 값으로 끝나면 선택\n",
    "# <요소>[<속성>*=<값>] = 해당 속성의 값이 지정한 값을 포함하고 있으면 선택\n",
    "## 위치 또는 상태를 지정하는 서식 =>\n",
    "# <요소>: root = 루트 요소, <요소>:nth-child(n) = n번째 자식 요소, <요소>:nth-last-child(n) = 뒤에서부터 n번째 자식 요소,\n",
    "# <요소>:nth-of-type(n) = n번쨰 해당 종류의 요소, <요소>:first-child = 첫 번쨰 자식 요소, <요소>:last-child = 마지막 번쨰 자식 요소,\n",
    "# <요소>:first-of-type = 첫 번째 해당 종류의 요소, <요소>:last-of-type = 마지막 번째 해당 종류의 요소, <요소>:only-child = 자식으로 유일한 요소\n",
    "# <요소>:only-of-type = 자식으로 유일한 종류의 요소, <요소>:empty = 내용이 없는 요소, <요소>:lang(code) = 특정 언어로 code를 지정한 요소,\n",
    "# <요소>:not(s) = s 이외의 요소, <요소>:enabled = 활성화된 UI요소, <요소>:disabled = 비활성화된 UI요소, <요소>:checked = 체크돼 있는 UI요소 선택\n",
    "\n",
    "\n",
    "for a in a_list:\n",
    "    name = a.string\n",
    "    print(\"-\",name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sel-books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers\n",
      "Numbers\n",
      "Numbers\n",
      "Numbers\n",
      "Numbers\n",
      "Numbers\n",
      "Numbers\n",
      "Numbers\n",
      "Numbers\n",
      "Numbers\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "books = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <ul id=\"bible\">\n",
    "            <li id=\"ge\">Genesis</li>\n",
    "            <li id=\"ex\">Exodus</li>\n",
    "            <li id=\"le\">Leviticus</li>\n",
    "            <li id=\"nu\">Numbers</li>\n",
    "            <li id=\"de\">Deuteronomy</li>\n",
    "        </ul>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(books, \"html.parser\")\n",
    "\n",
    "# CSS 선택자로 검색하는 방법\n",
    "sel = lambda q: print(soup.select_one(q).string)\n",
    "# lambda 인자 : 표현식\n",
    "# sel(\"a\") = a에 대해 print(soup.select_one(a).string) 한 것을 q에다 집어넣는다.\n",
    "sel(\"#nu\")\n",
    "# id 속성이 nu인 것을 추출\n",
    "sel(\"li#nu\")\n",
    "# li 태그를 추가하여 지정\n",
    "sel(\"ul > li#nu\")\n",
    "# ul 태그의 자식이라는 것을 추가\n",
    "sel(\"#bible #nu\")\n",
    "# #bible 아래의 #nu를 선택\n",
    "sel(\"#bible > #nu\")\n",
    "# 직접적인 부모 자식 관계를 가지고 있다는 것을 나타냄\n",
    "sel(\"ul#bible > li#nu\")\n",
    "# id가 bible인 ul 태그 바로 아래에 있는 id가 nu인 li태그를 선택\n",
    "sel(\"li[id='nu']\")\n",
    "# 속성 검색을 사용해 id가 nu인 li태그를 지정\n",
    "sel(\"li:nth-of-type(4)\")\n",
    "# 4번째 li 태그를 추출\n",
    "\n",
    "# 그 밖의 방법\n",
    "print(soup.select(\"li\")[3].string)\n",
    "# soup.select로 모든 li 태그를 추출하고 3번째 태그를 불러옴\n",
    "print(soup.find_all(\"li\")[3].string)\n",
    "# soup.find_all로 모든 li 태그를 추출하고 3번째 태그를 불러옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sel-avocado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아보카도\n",
      "아보카도\n",
      "아보카도\n",
      "아보카도\n",
      "아보카도\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "fp = open(\"fruits-vegetables.html\", encoding=\"utf-8\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "\n",
    "# CSS 선택자로 추출하기 -- (*1)\n",
    "print(soup.select(\"ul#ve-list > li[data-lo='us']\")[1].string)\n",
    "# id가 ve-list인 요소 바로 아래에 있는 <li> 태그 중에서 data-lo 속성이 'us'인 것을 모두 추출하고, 그중에서 [1]을 선택\n",
    "print(soup.select(\"ul#ve-list > li.black\")[1].string)\n",
    "# class 속성이 black인 요소 가운데 [1]\n",
    "print(soup.select_one(\"#ve-list > li:nth-of-type(4)\").string)\n",
    "# id가 ve-list인 요소 바로 아래에 있는 <li> 태그 중에 4번째 요소\n",
    "\n",
    "# find 메서드로 추출하기 -- (*2)\n",
    "cond = {\"data-lo\":\"us\", \"class\":\"black\"}\n",
    "# data-lo 속성이 'us', class 속성이 \"black\" 인 것을 추출\n",
    "print(soup.find(\"li\",cond).string)\n",
    "\n",
    "# find 메서드를 연속적으로 사용하기 -- (*3)\n",
    "print(soup.find(id=\"ve-list\").find(\"li\", cond).string)\n",
    "# find 메서드를 두 번 조합, id가 ve-list인 요소에서 li 태그인 것을 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sel-re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://example.com/fuga\n",
      "https://example.com/foo\n",
      "https://example.com/aaa\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re # 정규 표현식을 사용할 때 -- (*1)\n",
    "\n",
    "html = \"\"\"\n",
    "<ul>\n",
    "    <li><a href=\"hoge.html\">hoge</li>\n",
    "    <li><a href=\"https://example.com/fuga\">fuga*</li>\n",
    "    <li><a href=\"https://example.com/foo\">foo*</li>\n",
    "    <li><a href=\"https://example.com/aaa\">aaa</li>\n",
    "</ul> \n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# 정규 표현식으로 href에서 https인 것 추출하기 -- (*2)\n",
    "li = soup.find_all(href=re.compile(r\"^https://\"))\n",
    "# 특정 패턴의 문자열을 검색하는데 매우 유용하다.\n",
    "# ex) 전화번호 검색 => re.compile(r`\\d\\d\\d=\\d\\d\\d-\\d\\d\\d\\d') (d는 digit = 0~9 아무 숫자), ^ = 이 패턴으로 시작해야 함, $ = 이 패턴으로 종료되어야 함,\n",
    "# [문자들] = 문자들 중에 하나이여야 함, [^문자들] = 피해야할 문자들의 집합, | = 두 패턴 중 하나이어야 함(or), ? = 앞 패턴이 없거나 하나이어야 함.\n",
    "# + = 앞 패턴이 하나 이상이어야 함, * = 앞 패턴이 0개 이상이어야 함, 패턴{n} = 앞 패턴이 n번 반복해서 나타나는 경우,\n",
    "# 패턴{n, m} = 앞 패턴이 최소 n번, 최대 m번 반복해서 나타나는 경우, \\d = 숫자 0~9, \\w = 문자를 의미, \\s = 화이트 스페이스 => [\\t\\n\\r\\f],\n",
    "# . = 뉴라인(\\n)을 제외한 모든 문자를 의미\n",
    "\n",
    "for e in li: print(e.attrs['href'])\n",
    "# attrs = 속성, ex) href"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1장 크롤링과 스크레이핑\n",
    "1-4 링크에 있는 것을 한꺼번에 내려받기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cr-path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/html/b.html\n",
      "http://example.com/html/sub/c.html\n",
      "http://example.com/index.html\n",
      "http://example.com/img/hpe.png\n",
      "http://example.com/css/hope.css\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "base = \"http://example.com/html/a.html\"\n",
    "\n",
    "print( urljoin(base, \"b.html\") )\n",
    "print( urljoin(base, \"sub/c.html\") )\n",
    "print( urljoin(base, \"../index.html\") )\n",
    "print( urljoin(base, \"../img/hpe.png\") )\n",
    "print( urljoin(base, \"../css/hope.css\") )\n",
    "# ../b.html 로 하면 http://example.com/b.html 로 된다.\n",
    "# url을 기반으로 상대 경로를 절대 경로로 변환한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cr-path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/hope.html\n",
      "http://otherExample.com/wiki\n",
      "http://anotherExample.org/test\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "base = \"http://example.com/html/a.html\"\n",
    "\n",
    "print( urljoin(base, \"/hope.html\") )\n",
    "print( urljoin(base, \"http://otherExample.com/wiki\") )\n",
    "print( urljoin(base, \"//anotherExample.org/test\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cr-getall\n",
    "순서 1) HTML 분석 2) 링크 추출 3) 각 링크 대상에 다음과 같은 처리 4) 파일을 다운 5) 파일이 html이라면 재귀적으로 (1)로 돌아가서 순서를 처음부터 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_html= https://docs.python.org/3.8/library/\n",
      "analyze_html= https://docs.python.org/3.8/library/intro.html\n",
      "analyze_html= https://docs.python.org/3.8/library/functions.html\n",
      "analyze_html= https://docs.python.org/3.8/library/constants.html\n",
      "analyze_html= https://docs.python.org/3.8/library/stdtypes.html\n",
      "analyze_html= https://docs.python.org/3.8/library/exceptions.html\n",
      "analyze_html= https://docs.python.org/3.8/library/text.html\n",
      "analyze_html= https://docs.python.org/3.8/library/string.html\n",
      "analyze_html= https://docs.python.org/3.8/library/re.html\n",
      "analyze_html= https://docs.python.org/3.8/library/difflib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/textwrap.html\n",
      "analyze_html= https://docs.python.org/3.8/library/unicodedata.html\n",
      "analyze_html= https://docs.python.org/3.8/library/stringprep.html\n",
      "analyze_html= https://docs.python.org/3.8/library/readline.html\n",
      "analyze_html= https://docs.python.org/3.8/library/rlcompleter.html\n",
      "analyze_html= https://docs.python.org/3.8/library/binary.html\n",
      "analyze_html= https://docs.python.org/3.8/library/struct.html\n",
      "analyze_html= https://docs.python.org/3.8/library/codecs.html\n",
      "analyze_html= https://docs.python.org/3.8/library/datatypes.html\n",
      "analyze_html= https://docs.python.org/3.8/library/datetime.html\n",
      "analyze_html= https://docs.python.org/3.8/library/calendar.html\n",
      "analyze_html= https://docs.python.org/3.8/library/collections.html\n",
      "analyze_html= https://docs.python.org/3.8/library/collections.abc.html\n",
      "analyze_html= https://docs.python.org/3.8/library/heapq.html\n",
      "analyze_html= https://docs.python.org/3.8/library/bisect.html\n",
      "analyze_html= https://docs.python.org/3.8/library/array.html\n",
      "analyze_html= https://docs.python.org/3.8/library/weakref.html\n",
      "analyze_html= https://docs.python.org/3.8/library/types.html\n",
      "analyze_html= https://docs.python.org/3.8/library/copy.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pprint.html\n",
      "analyze_html= https://docs.python.org/3.8/library/reprlib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/enum.html\n",
      "analyze_html= https://docs.python.org/3.8/library/numeric.html\n",
      "analyze_html= https://docs.python.org/3.8/library/numbers.html\n",
      "analyze_html= https://docs.python.org/3.8/library/math.html\n",
      "analyze_html= https://docs.python.org/3.8/library/cmath.html\n",
      "analyze_html= https://docs.python.org/3.8/library/decimal.html\n",
      "analyze_html= https://docs.python.org/3.8/library/fractions.html\n",
      "analyze_html= https://docs.python.org/3.8/library/random.html\n",
      "analyze_html= https://docs.python.org/3.8/library/statistics.html\n",
      "analyze_html= https://docs.python.org/3.8/library/functional.html\n",
      "analyze_html= https://docs.python.org/3.8/library/itertools.html\n",
      "analyze_html= https://docs.python.org/3.8/library/functools.html\n",
      "analyze_html= https://docs.python.org/3.8/library/operator.html\n",
      "analyze_html= https://docs.python.org/3.8/library/filesys.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pathlib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/os.path.html\n",
      "analyze_html= https://docs.python.org/3.8/library/fileinput.html\n",
      "analyze_html= https://docs.python.org/3.8/library/stat.html\n",
      "analyze_html= https://docs.python.org/3.8/library/filecmp.html\n",
      "analyze_html= https://docs.python.org/3.8/library/tempfile.html\n",
      "analyze_html= https://docs.python.org/3.8/library/glob.html\n",
      "analyze_html= https://docs.python.org/3.8/library/fnmatch.html\n",
      "analyze_html= https://docs.python.org/3.8/library/linecache.html\n",
      "analyze_html= https://docs.python.org/3.8/library/shutil.html\n",
      "analyze_html= https://docs.python.org/3.8/library/persistence.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pickle.html\n",
      "analyze_html= https://docs.python.org/3.8/library/copyreg.html\n",
      "analyze_html= https://docs.python.org/3.8/library/shelve.html\n",
      "analyze_html= https://docs.python.org/3.8/library/marshal.html\n",
      "analyze_html= https://docs.python.org/3.8/library/dbm.html\n",
      "analyze_html= https://docs.python.org/3.8/library/sqlite3.html\n",
      "analyze_html= https://docs.python.org/3.8/library/archiving.html\n",
      "analyze_html= https://docs.python.org/3.8/library/zlib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/gzip.html\n",
      "analyze_html= https://docs.python.org/3.8/library/bz2.html\n",
      "analyze_html= https://docs.python.org/3.8/library/lzma.html\n",
      "analyze_html= https://docs.python.org/3.8/library/zipfile.html\n",
      "analyze_html= https://docs.python.org/3.8/library/tarfile.html\n",
      "analyze_html= https://docs.python.org/3.8/library/fileformats.html\n",
      "analyze_html= https://docs.python.org/3.8/library/csv.html\n",
      "analyze_html= https://docs.python.org/3.8/library/configparser.html\n",
      "analyze_html= https://docs.python.org/3.8/library/netrc.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xdrlib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/plistlib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/crypto.html\n",
      "analyze_html= https://docs.python.org/3.8/library/hashlib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/hmac.html\n",
      "analyze_html= https://docs.python.org/3.8/library/secrets.html\n",
      "analyze_html= https://docs.python.org/3.8/library/allos.html\n",
      "analyze_html= https://docs.python.org/3.8/library/os.html\n",
      "analyze_html= https://docs.python.org/3.8/library/io.html\n",
      "analyze_html= https://docs.python.org/3.8/library/time.html\n",
      "analyze_html= https://docs.python.org/3.8/library/argparse.html\n",
      "analyze_html= https://docs.python.org/3.8/library/getopt.html\n",
      "analyze_html= https://docs.python.org/3.8/library/logging.html\n",
      "analyze_html= https://docs.python.org/3.8/library/logging.config.html\n",
      "analyze_html= https://docs.python.org/3.8/library/logging.handlers.html\n",
      "analyze_html= https://docs.python.org/3.8/library/getpass.html\n",
      "analyze_html= https://docs.python.org/3.8/library/curses.html\n",
      "analyze_html= https://docs.python.org/3.8/library/curses.ascii.html\n",
      "analyze_html= https://docs.python.org/3.8/library/curses.panel.html\n",
      "analyze_html= https://docs.python.org/3.8/library/platform.html\n",
      "analyze_html= https://docs.python.org/3.8/library/errno.html\n",
      "analyze_html= https://docs.python.org/3.8/library/ctypes.html\n",
      "analyze_html= https://docs.python.org/3.8/library/concurrency.html\n",
      "analyze_html= https://docs.python.org/3.8/library/threading.html\n",
      "analyze_html= https://docs.python.org/3.8/library/multiprocessing.html\n",
      "analyze_html= https://docs.python.org/3.8/library/multiprocessing.shared_memory.html\n",
      "analyze_html= https://docs.python.org/3.8/library/concurrent.html\n",
      "analyze_html= https://docs.python.org/3.8/library/concurrent.futures.html\n",
      "analyze_html= https://docs.python.org/3.8/library/subprocess.html\n",
      "analyze_html= https://docs.python.org/3.8/library/sched.html\n",
      "analyze_html= https://docs.python.org/3.8/library/queue.html\n",
      "analyze_html= https://docs.python.org/3.8/library/_thread.html\n",
      "analyze_html= https://docs.python.org/3.8/library/_dummy_thread.html\n",
      "analyze_html= https://docs.python.org/3.8/library/dummy_threading.html\n",
      "analyze_html= https://docs.python.org/3.8/library/contextvars.html\n",
      "analyze_html= https://docs.python.org/3.8/library/ipc.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-task.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-stream.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-sync.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-subprocess.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-queue.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-exceptions.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-eventloop.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-future.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-protocol.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-policy.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-platforms.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-api-index.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-llapi-index.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncio-dev.html\n",
      "analyze_html= https://docs.python.org/3.8/library/socket.html\n",
      "analyze_html= https://docs.python.org/3.8/library/ssl.html\n",
      "analyze_html= https://docs.python.org/3.8/library/select.html\n",
      "analyze_html= https://docs.python.org/3.8/library/selectors.html\n",
      "analyze_html= https://docs.python.org/3.8/library/asyncore.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_html= https://docs.python.org/3.8/library/asynchat.html\n",
      "analyze_html= https://docs.python.org/3.8/library/signal.html\n",
      "analyze_html= https://docs.python.org/3.8/library/mmap.html\n",
      "analyze_html= https://docs.python.org/3.8/library/netdata.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.message.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.parser.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.generator.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.policy.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.errors.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.headerregistry.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.contentmanager.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.examples.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.compat32-message.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.mime.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.header.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.charset.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.encoders.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.utils.html\n",
      "analyze_html= https://docs.python.org/3.8/library/email.iterators.html\n",
      "analyze_html= https://docs.python.org/3.8/library/json.html\n",
      "analyze_html= https://docs.python.org/3.8/library/mailcap.html\n",
      "analyze_html= https://docs.python.org/3.8/library/mailbox.html\n",
      "analyze_html= https://docs.python.org/3.8/library/mimetypes.html\n",
      "analyze_html= https://docs.python.org/3.8/library/base64.html\n",
      "analyze_html= https://docs.python.org/3.8/library/binhex.html\n",
      "analyze_html= https://docs.python.org/3.8/library/binascii.html\n",
      "analyze_html= https://docs.python.org/3.8/library/quopri.html\n",
      "analyze_html= https://docs.python.org/3.8/library/uu.html\n",
      "analyze_html= https://docs.python.org/3.8/library/markup.html\n",
      "analyze_html= https://docs.python.org/3.8/library/html.html\n",
      "analyze_html= https://docs.python.org/3.8/library/html.parser.html\n",
      "analyze_html= https://docs.python.org/3.8/library/html.entities.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xml.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xml.etree.elementtree.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xml.dom.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xml.dom.minidom.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xml.dom.pulldom.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xml.sax.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xml.sax.handler.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xml.sax.utils.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xml.sax.reader.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pyexpat.html\n",
      "analyze_html= https://docs.python.org/3.8/library/internet.html\n",
      "analyze_html= https://docs.python.org/3.8/library/webbrowser.html\n",
      "analyze_html= https://docs.python.org/3.8/library/cgi.html\n",
      "analyze_html= https://docs.python.org/3.8/library/cgitb.html\n",
      "analyze_html= https://docs.python.org/3.8/library/wsgiref.html\n",
      "analyze_html= https://docs.python.org/3.8/library/urllib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/urllib.request.html\n",
      "analyze_html= https://docs.python.org/3.8/library/urllib.parse.html\n",
      "analyze_html= https://docs.python.org/3.8/library/urllib.error.html\n",
      "analyze_html= https://docs.python.org/3.8/library/urllib.robotparser.html\n",
      "analyze_html= https://docs.python.org/3.8/library/http.html\n",
      "analyze_html= https://docs.python.org/3.8/library/http.client.html\n",
      "analyze_html= https://docs.python.org/3.8/library/ftplib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/poplib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/imaplib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/nntplib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/smtplib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/smtpd.html\n",
      "analyze_html= https://docs.python.org/3.8/library/telnetlib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/uuid.html\n",
      "analyze_html= https://docs.python.org/3.8/library/socketserver.html\n",
      "analyze_html= https://docs.python.org/3.8/library/http.server.html\n",
      "analyze_html= https://docs.python.org/3.8/library/http.cookies.html\n",
      "analyze_html= https://docs.python.org/3.8/library/http.cookiejar.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xmlrpc.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xmlrpc.client.html\n",
      "analyze_html= https://docs.python.org/3.8/library/xmlrpc.server.html\n",
      "analyze_html= https://docs.python.org/3.8/library/ipaddress.html\n",
      "analyze_html= https://docs.python.org/3.8/library/mm.html\n",
      "analyze_html= https://docs.python.org/3.8/library/audioop.html\n",
      "analyze_html= https://docs.python.org/3.8/library/aifc.html\n",
      "analyze_html= https://docs.python.org/3.8/library/sunau.html\n",
      "analyze_html= https://docs.python.org/3.8/library/wave.html\n",
      "analyze_html= https://docs.python.org/3.8/library/chunk.html\n",
      "analyze_html= https://docs.python.org/3.8/library/colorsys.html\n",
      "analyze_html= https://docs.python.org/3.8/library/imghdr.html\n",
      "analyze_html= https://docs.python.org/3.8/library/sndhdr.html\n",
      "analyze_html= https://docs.python.org/3.8/library/ossaudiodev.html\n",
      "analyze_html= https://docs.python.org/3.8/library/i18n.html\n",
      "analyze_html= https://docs.python.org/3.8/library/gettext.html\n",
      "analyze_html= https://docs.python.org/3.8/library/locale.html\n",
      "analyze_html= https://docs.python.org/3.8/library/frameworks.html\n",
      "analyze_html= https://docs.python.org/3.8/library/turtle.html\n",
      "analyze_html= https://docs.python.org/3.8/library/cmd.html\n",
      "analyze_html= https://docs.python.org/3.8/library/shlex.html\n",
      "analyze_html= https://docs.python.org/3.8/library/tk.html\n",
      "analyze_html= https://docs.python.org/3.8/library/tkinter.html\n",
      "analyze_html= https://docs.python.org/3.8/library/tkinter.ttk.html\n",
      "analyze_html= https://docs.python.org/3.8/library/tkinter.tix.html\n",
      "analyze_html= https://docs.python.org/3.8/library/tkinter.scrolledtext.html\n",
      "analyze_html= https://docs.python.org/3.8/library/idle.html\n",
      "analyze_html= https://docs.python.org/3.8/library/othergui.html\n",
      "analyze_html= https://docs.python.org/3.8/library/development.html\n",
      "analyze_html= https://docs.python.org/3.8/library/typing.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pydoc.html\n",
      "analyze_html= https://docs.python.org/3.8/library/doctest.html\n",
      "analyze_html= https://docs.python.org/3.8/library/unittest.html\n",
      "analyze_html= https://docs.python.org/3.8/library/unittest.mock.html\n",
      "analyze_html= https://docs.python.org/3.8/library/unittest.mock-examples.html\n",
      "analyze_html= https://docs.python.org/3.8/library/2to3.html\n",
      "analyze_html= https://docs.python.org/3.8/library/test.html\n",
      "analyze_html= https://docs.python.org/3.8/library/debug.html\n",
      "analyze_html= https://docs.python.org/3.8/library/audit_events.html\n",
      "analyze_html= https://docs.python.org/3.8/library/bdb.html\n",
      "analyze_html= https://docs.python.org/3.8/library/faulthandler.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pdb.html\n",
      "analyze_html= https://docs.python.org/3.8/library/profile.html\n",
      "analyze_html= https://docs.python.org/3.8/library/timeit.html\n",
      "analyze_html= https://docs.python.org/3.8/library/trace.html\n",
      "analyze_html= https://docs.python.org/3.8/library/tracemalloc.html\n",
      "analyze_html= https://docs.python.org/3.8/library/distribution.html\n",
      "analyze_html= https://docs.python.org/3.8/library/distutils.html\n",
      "analyze_html= https://docs.python.org/3.8/library/ensurepip.html\n",
      "analyze_html= https://docs.python.org/3.8/library/venv.html\n",
      "analyze_html= https://docs.python.org/3.8/library/zipapp.html\n",
      "analyze_html= https://docs.python.org/3.8/library/python.html\n",
      "analyze_html= https://docs.python.org/3.8/library/sys.html\n",
      "analyze_html= https://docs.python.org/3.8/library/sysconfig.html\n",
      "analyze_html= https://docs.python.org/3.8/library/builtins.html\n",
      "analyze_html= https://docs.python.org/3.8/library/__main__.html\n",
      "analyze_html= https://docs.python.org/3.8/library/warnings.html\n",
      "analyze_html= https://docs.python.org/3.8/library/dataclasses.html\n",
      "analyze_html= https://docs.python.org/3.8/library/contextlib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/abc.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyze_html= https://docs.python.org/3.8/library/atexit.html\n",
      "analyze_html= https://docs.python.org/3.8/library/traceback.html\n",
      "analyze_html= https://docs.python.org/3.8/library/__future__.html\n",
      "analyze_html= https://docs.python.org/3.8/library/gc.html\n",
      "analyze_html= https://docs.python.org/3.8/library/inspect.html\n",
      "analyze_html= https://docs.python.org/3.8/library/site.html\n",
      "analyze_html= https://docs.python.org/3.8/library/custominterp.html\n",
      "analyze_html= https://docs.python.org/3.8/library/code.html\n",
      "analyze_html= https://docs.python.org/3.8/library/codeop.html\n",
      "analyze_html= https://docs.python.org/3.8/library/modules.html\n",
      "analyze_html= https://docs.python.org/3.8/library/zipimport.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pkgutil.html\n",
      "analyze_html= https://docs.python.org/3.8/library/modulefinder.html\n",
      "analyze_html= https://docs.python.org/3.8/library/runpy.html\n",
      "analyze_html= https://docs.python.org/3.8/library/importlib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/importlib.metadata.html\n",
      "analyze_html= https://docs.python.org/3.8/library/language.html\n",
      "analyze_html= https://docs.python.org/3.8/library/parser.html\n",
      "analyze_html= https://docs.python.org/3.8/library/ast.html\n",
      "analyze_html= https://docs.python.org/3.8/library/symtable.html\n",
      "analyze_html= https://docs.python.org/3.8/library/symbol.html\n",
      "analyze_html= https://docs.python.org/3.8/library/token.html\n",
      "analyze_html= https://docs.python.org/3.8/library/keyword.html\n",
      "analyze_html= https://docs.python.org/3.8/library/tokenize.html\n",
      "analyze_html= https://docs.python.org/3.8/library/tabnanny.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pyclbr.html\n",
      "analyze_html= https://docs.python.org/3.8/library/py_compile.html\n",
      "analyze_html= https://docs.python.org/3.8/library/compileall.html\n",
      "analyze_html= https://docs.python.org/3.8/library/dis.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pickletools.html\n",
      "analyze_html= https://docs.python.org/3.8/library/misc.html\n",
      "analyze_html= https://docs.python.org/3.8/library/formatter.html\n",
      "analyze_html= https://docs.python.org/3.8/library/windows.html\n",
      "analyze_html= https://docs.python.org/3.8/library/msilib.html\n",
      "analyze_html= https://docs.python.org/3.8/library/msvcrt.html\n",
      "analyze_html= https://docs.python.org/3.8/library/winreg.html\n",
      "analyze_html= https://docs.python.org/3.8/library/winsound.html\n",
      "analyze_html= https://docs.python.org/3.8/library/unix.html\n",
      "analyze_html= https://docs.python.org/3.8/library/posix.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pwd.html\n",
      "analyze_html= https://docs.python.org/3.8/library/spwd.html\n",
      "analyze_html= https://docs.python.org/3.8/library/grp.html\n",
      "analyze_html= https://docs.python.org/3.8/library/crypt.html\n",
      "analyze_html= https://docs.python.org/3.8/library/termios.html\n",
      "analyze_html= https://docs.python.org/3.8/library/tty.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pty.html\n",
      "analyze_html= https://docs.python.org/3.8/library/fcntl.html\n",
      "analyze_html= https://docs.python.org/3.8/library/pipes.html\n",
      "analyze_html= https://docs.python.org/3.8/library/resource.html\n",
      "analyze_html= https://docs.python.org/3.8/library/nis.html\n",
      "analyze_html= https://docs.python.org/3.8/library/syslog.html\n",
      "analyze_html= https://docs.python.org/3.8/library/superseded.html\n",
      "analyze_html= https://docs.python.org/3.8/library/optparse.html\n",
      "analyze_html= https://docs.python.org/3.8/library/imp.html\n",
      "analyze_html= https://docs.python.org/3.8/library/undoc.html\n"
     ]
    }
   ],
   "source": [
    "# 파이썬 매뉴얼을 재귀적으로 다운받는 프로그램\n",
    "# 모듈 읽어 들이기  -- (*1)\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import *\n",
    "# 인터넷에서 데이터를 내려받는 것\n",
    "from urllib.parse import *\n",
    "# URL 분석\n",
    "from os import makedirs\n",
    "# 폴더 생성을 위한 os\n",
    "import os.path, time, re\n",
    "# 경로와 관련된 os.path, 슬립(잠시 쉬는 것)을 위한 time, 정규 표현식을 위한 re\n",
    "\n",
    "# 이미 처리한 파일인지 확인하기 위한 변수 -- (*2)\n",
    "proc_files = {}\n",
    "# 이미 분석한 html 파일인지 판별하기 위한 변수\n",
    "# a -> b로 이동하는 링크가 있으면 b -> a 링크로 이동하는 변수가 있을 수 있다. 그래서 이를 따로 처리하기 위해 종료해주기 위함\n",
    "\n",
    "# HTML 내부에 있는 링크를 추출하는 함수 -- (*3)\n",
    "def enum_links(html, base): # enum 함수에서는 html을 분석하고 링크를 추출\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = soup.select(\"link[rel='stylesheet']\") #CSS # link태그로 스타일시트의 경로를 찾는다.\n",
    "    links += soup.select(\"a[href]\") # 링크 # a태그로 링크를 찾는다.\n",
    "    result = []\n",
    "    # href 속성을 추출하고, 링크를 절대 경로로 변환 -- (*4)\n",
    "    # 링크 태그의 href 속성에 적혀 있는 url을 추출하고 절대 경로로 변환\n",
    "    for a in links:\n",
    "        href = a.attrs['href']\n",
    "        url = urljoin(base, href)\n",
    "        result.append(url) # append = 그 자체를 원소로 넣고, expend = iterable로 넣는다.\n",
    "    return result\n",
    "\n",
    "# 파일을 다운받고 저장하는 함수 -- (*5)\n",
    "# 인터넷에 있는 파일을 다운받는 부분, url을 기반으로 파일명을 결정하고 필요하다면 폴더를 생성\n",
    "def download_file(url):\n",
    "    o = urlparse(url) # urlparse = url을 분해, 조립, 변경 등을 처리하는 함수를 제공\n",
    "    savepath = \"./\" + o.netloc + o.path # netloc = 네트워크의 위치, user:password@host:port 형식으로 표현되며, http의 경우 host:port로 됨\n",
    "    # path = 파일이나 애플리케이션 경로를 의미, scheme = url에 사용된 프로토콜을 의미, params = 애플리케이션에 전달된 매개변수\n",
    "    if re.search(r\"/$\", savepath): # 폴더라면 index.html\n",
    "        savepath += \"index.html\"\n",
    "    savedir = os.path.dirname(savepath) # os.path.dirname = path의 파일/디렉토리를 반환\n",
    "    # 모두 다운됐는지 확인\n",
    "    if os.path.exists(savepath): return savepath\n",
    "    # 다운받을 폴더 생성\n",
    "    if not os.path.exists(savedir):\n",
    "        print(\"mkdir=\",savedir)\n",
    "        makedirs(savedir) # makedirs = 디렉토리를 생성 \n",
    "    # 파일 다운받기 -- (*6)\n",
    "    try:\n",
    "        print(\"download=\", url)\n",
    "        urlretrieve(url, savepath) # 실제로 다운로드할 때는 urlretrieve를 사용\n",
    "        time.sleep(1) # 1초 휴식 -- (*7)\n",
    "        return savepath\n",
    "    except:\n",
    "        print(\"다운 실패:\", url)\n",
    "        return None\n",
    "    \n",
    "# HTML을 분석하고 다운받는 함수 -- (*8)\n",
    "def analyze_html(url, root_url): # analyze_html함수는 html 파일을 분석하고 링크에 있는 것을 다운\n",
    "    savepath = download_file(url)\n",
    "    if savepath is None: return\n",
    "    if savepath in proc_files: return # 이미 처리됐다면 실행하지 않음 -- (*9) # 같은 파일에 반복하지 않게 확인하는 부분\n",
    "    proc_files[savepath] = True\n",
    "    print(\"analyze_html=\", url)\n",
    "    # 링크 추출 -- (*10) \n",
    "    html = open(savepath, \"r\", encoding=\"utf-8\").read()\n",
    "    links = enum_links(html, url) # enum_links = html을 분석하고 링크를 추출\n",
    "    \n",
    "    for link_url in links:\n",
    "        # 링크가 루트 이외의 경로를 나타낸다면 무시 -- (*11) # 링크 대상을 확인해서 링크가 해당 사이트가 아닌 경우 다룬로드 하지않게\n",
    "        if link_url.find(root_url) != 0: # css파일을 다운로드하지 않으면 레이아웃이 깨질 수 있으므로 css파일은 예외적으로 다운로드\n",
    "            if not re.search(r\".css$\", link_url): continue\n",
    "        # HTML 이라면\n",
    "        if re.search(r\".(html|htm)$\", link_url):\n",
    "            # 재귀적으로 HTML 파일 분석하기\n",
    "            analyze_html(link_url, root_url)\n",
    "            continue\n",
    "        # 기타 파일\n",
    "        download_file(link_url)\n",
    "        \n",
    "if __name__ == \"__main__\": # __name__에는 모듈 이름이 들어오게 되는데, 모듈이 아닌 경우에 __main__이 들어온다.\n",
    "    # 따라서 이 스크립트를 직접적으로 실행한 경우에만 __name__에 __main이 들어와서 처리를 진행하게 된다.\n",
    "    # URL에 있는 모든 것 다운받기 -- (*12) # 어떤 사이트를 다운로드할지 지정\n",
    "    url = \"https://docs.python.org/3.8/library/\"\n",
    "    analyze_html(url, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### login-getmileage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "마일리지: 2,000\n",
      "이코인: 0\n"
     ]
    }
   ],
   "source": [
    "# 로그인을 위한 모듈 추출하기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 아이디와 비밀번호 지정하기 -- (*1)\n",
    "USER = \"dkssud8150\"\n",
    "PASS = \"dkssud881!\"\n",
    "\n",
    "# 세션 시작하기 -- (*2)\n",
    "session = requests.session()\n",
    "# rsquests.session() = session을 생성\n",
    "\n",
    "# 로그인하기 -- (*3)\n",
    "login_info = { # parameter 형태 => 자료형 이름 = {\"param1\":\"value1\",\"param2\":\"value2\",...}\n",
    "    \"m_id\": USER, # 아이디 지정, html에서 'name:m_id'로 된 것을 불러옴 = post형식\n",
    "    \"m_passwd\": PASS # 비밀번호 지정\n",
    "}\n",
    "# parameter로 직접 넣어서 보내기 => url=\"http://www.naver.com?a=b&bb=123\", res=requests.get(url)\n",
    "# dictionary 사용 => paramDict = {\"a\":\"b\",\"bb\":123} , url=\"http://www.naver.com\", res=requests.get(url, params=paramDict)\n",
    "\n",
    "url_login = \"https://www.hanbit.co.kr/member/login_proc.php\"\n",
    "res = session.post(url_login, data=login_info) # POST방식으로 된 데이터 요청, post로 넣을때는 파라미터를 넣지 않아서 dictionary 사용해야 함\n",
    "# data를 딕셔너리 구조로 유지하면서 문자열로 바꿔서 전달해야 한다. 이때 사용하는 것이 json 모듈 => \n",
    "# json.dumps() {s를 빼먹으면 오류남} = json 포멧 데이터로 바꿔서 파이썬에서 계속 작업하기 위함.\n",
    "res.raise_for_status() # 오류가 발생하면 예외가 발생합니다.\n",
    "# res.request = 내가 보낸 request 객체에 접근 가능, res.status_code = 응답 코드, \n",
    "# res.raise_for_status = 200 ok 코드가 아닌 경우 에러 발동, res.json = json response일 경우 딕셔너리 타입으로 바로 변환\n",
    "\n",
    "# 마이페이지에 접근하기 -- (*4)\n",
    "url_mypage = \"https://www.hanbit.co.kr/myhanbit/myhanbit.html\" # 마이페이지 주소\n",
    "res = session.get(url_mypage)\n",
    "res.raise_for_status()\n",
    "\n",
    "# 마일리지와 이코인 가져오기 -- (*5)\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "mileage = soup.select_one(\".mileage_section1 span\").get_text() \n",
    "# get_text = 현재 태그를 포함하여 모든 하위 태그를 제거하고 유니코드 텍스트만 들어있는 문자열을 반환\n",
    "ecoin = soup.select_one(\".mileage_section2 span\").get_text()\n",
    "print(\"마일리지: \" + mileage)\n",
    "print(\"이코인: \" + ecoin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### requests-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/08/19 01:16:10\n",
      "b'2020/08/19 01:16:10'\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "import requests\n",
    "r = requests.get(\"http://api.aoikujira.com/time/get.php\")\n",
    "\n",
    "# 텍스트 형식으로 데이터 추출하기\n",
    "text = r.text\n",
    "print(text) # 일반 텍스트\n",
    "\n",
    "# 바이너리 형식으로 데이터 추출하기\n",
    "bin = r.content\n",
    "print(bin) # b''형태 => b''는 파이썬에서 바이너리라는 것을 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### request-png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "# 바이너리를 활용해서 바이너리 데이터인 이미지를 받아 저장하는 예제\n",
    "# 이미지 데이터 추출하기\n",
    "import requests\n",
    "r = requests.get(\"http://wikibook.co.kr/wikibook.png\")\n",
    "\n",
    "# 바이너리 형식으로 데이터 저장하기\n",
    "with open(\"test.png\", \"wb\") as f: # 쓰기 형태의 바이너리 파일로 연다.\n",
    "    f.write(r.content) # content = 바이트로 리턴함\n",
    "    \n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2장 고급 스크레이핑\n",
    "2-2 웹 브라우저를 이용한 스크레이핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
