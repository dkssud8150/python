{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = open(\"data_ch1/file.txt\", \"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentence = open(\"data_ch1/file.txt\", \"r\").read()\n",
    "\n",
    "sent_list  = sent_tokenize(sentence)\n",
    "len(sent_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you want to get early access of it, you should book your order now.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "sent = \"\"\"\n",
    "In this book authored by Sohom Ghosh and Dwight Gunning, .. insights from it. \n",
    "The first four chapter will introduce you to the basics of NLP. \n",
    "Later chapters will describe how to deal with complex NLP prajects. \n",
    "If you want to get early access of it, you should book your order now.\n",
    "\"\"\"\n",
    "\n",
    "lTokenizer = LineTokenizer()\n",
    "sent_list = lTokenizer.tokenize(sent)\n",
    "sent_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this book authored by Sohom Ghosh and Dwight Gunning, we shall learnning how to pracess Natueral Language and extract insights from it.\n",
      "\n",
      "['In', 'this', 'book', 'authored', 'by', 'Sohom', 'Ghosh', 'and', 'Dwight', 'Gunning', ',', 'we']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "print(sent_list[0])\n",
    "print()\n",
    "words = word_tokenize(sentence)\n",
    "print(words[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet Tokenizer output :\n",
      "['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tTokenizer = TweetTokenizer()\n",
    "print(\"Tweet Tokenizer output :\")\n",
    "print(tTokenizer.tokenize(\"This is a cooool #dummysmiley: :-) :-P <3\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "s1 = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging은 단어를 품사로 분류(categorize)하는 작업이다. 이렇게 분류하는 게 얼마나 효과적일까? 아래 예제를 통해 이런 의구심을 조금이나마 줄여보도록 하자. 'woman'(명사), 'bought'(동사), 'over'(전치사), 및 'the'(관사)에 대해 유사 단어를 찾아보자. text.similar() 메소드를 사용한다. 이 메소드는 w와 같은 문맥(w1-w-w2)을 갖는 유사 단어를 찾아준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day year car moment world house family child country boy\n",
      "state job place way war girl work word\n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())\n",
    "\n",
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made said done put had seen found given left heard was been brought\n",
      "set got that took in told felt\n"
     ]
    }
   ],
   "source": [
    "text.similar('bought')\n",
    "#text.similar('over')\n",
    "#text.similar('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this book authored by Sohom Ghosh and Dwight Gunning, we shall learnning how to pracess Natueral Language and extract insights from it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('book', 'NN'),\n",
       " ('authored', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Sohom', 'NNP'),\n",
       " ('Ghosh', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Dwight', 'NNP'),\n",
       " ('Gunning', 'NNP'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('shall', 'MD'),\n",
       " ('learnning', 'VB'),\n",
       " ('how', 'WRB'),\n",
       " ('to', 'TO'),\n",
       " ('pracess', 'VB'),\n",
       " ('Natueral', 'NNP'),\n",
       " ('Language', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('extract', 'JJ'),\n",
       " ('insights', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sent_list[0])\n",
    "\n",
    "print(sent_list[0])\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어(stop words) 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yshon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print(stopwords[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'book',\n",
       " 'authored',\n",
       " 'Sohom',\n",
       " 'Ghosh',\n",
       " 'Dwight',\n",
       " 'Gunning',\n",
       " ',',\n",
       " 'shall',\n",
       " 'learnning']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "\n",
    "new_words = [wd for wd in words if wd not in stop_words]\n",
    "new_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트 정규화 - 서로 다른 표현을 통일시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I visited United States from United Kingdom on 01-08-2020'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"I visited US from UK on 01-08-20\"\n",
    "normalized_sample = sample.replace(\"US\", \"United States\").replace(\"UK\",\"United Kingdom\").replace(\"-20\", \"-2020\")\n",
    "normalized_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "철자 수정 : pip install autocorrect 명령을 실행하여 autocorrect 모듈을 사전에 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not sleepy and there is no place I'm going to.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller()\n",
    "spell(\"I'm not sleapy and tehre is no place I'm giong to.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yshon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sohom has been corrected to: Soom\n",
      "Ghosh has been corrected to: Those\n",
      "learnning has been corrected to: learning\n",
      "pracess has been corrected to: process\n",
      "Natueral has been corrected to: Natural\n",
      "prajects has been corrected to: projects\n"
     ]
    }
   ],
   "source": [
    "corrected_sentence = \"\"\n",
    "corrected_words = []\n",
    "\n",
    "for wd in new_words:\n",
    "    tmp = wd\n",
    "    if wd not in string.punctuation:\n",
    "        wd_c = spell(wd)\n",
    "        if wd_c != wd:\n",
    "            print(wd + \" has been corrected to: \"+ wd_c)\n",
    "            tmp = wd_c\n",
    "       \n",
    "    corrected_sentence = corrected_sentence + \" \" + tmp\n",
    "    corrected_words.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'book', 'authored', 'Soom', 'Those', 'Dwight', 'Gunning', ',', 'shall', 'learning', 'process', 'Natural', 'Language', 'extract', 'insights', '.', 'The', 'first', 'four', 'chapter']\n"
     ]
    }
   ],
   "source": [
    "print(corrected_words[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어간 추출(Stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PorterStemmer, LancasterStemmer\n",
    "\n",
    "raw = \"\"\"\n",
    "My name is Maximus Decimus Meridius, commander of the armies of the north, \n",
    "General of the Felix legions and loyal servant to the true emperor, Marcus Aurelius. \n",
    "Father to a murdered son, husband to a murdered wife. And I will have my vengeance, in this life or the next.\n",
    "\"\"\"\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "stems = [porter.stem(t) for t in tokens]\n",
    "print(len(stems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    }
   ],
   "source": [
    "lancaster = LancasterStemmer()\n",
    "lstems = [lancaster.stem(t) for t in tokens]\n",
    "print(len(lstems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'book', 'author', 'soom', 'those', 'dwight', 'gun', ',', 'shall', 'learn', 'process', 'natur', 'languag', 'extract', 'insight', '.', 'the', 'first', 'four', 'chapter']\n"
     ]
    }
   ],
   "source": [
    "from nltk import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "corrected_words_stemmed = []\n",
    "for wd in corrected_words:\n",
    "    corrected_words_stemmed.append(porter.stem(wd))\n",
    "print(corrected_words_stemmed[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "walk\n",
      "meeting\n",
      "meet\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "wordlemma = WordNetLemmatizer()\n",
    "print(wordlemma.lemmatize('cars'))\n",
    "print(wordlemma.lemmatize('walking',pos='v'))\n",
    "print(wordlemma.lemmatize('meeting',pos='n'))\n",
    "print(wordlemma.lemmatize('meeting',pos='v'))\n",
    "print(wordlemma.lemmatize('better',pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Maximus', 'Decimus', 'Meridius', ',', 'commander', 'of', 'the', 'army', 'of', 'the', 'north', ',', 'General', 'of', 'the', 'Felix', 'legion', 'and', 'loyal', 'servant', 'to', 'the', 'true', 'emperor', ',', 'Marcus', 'Aurelius', '.', 'Father', 'to', 'a', 'murdered', 'son', ',', 'husband', 'to', 'a', 'murdered', 'wife', '.', 'And', 'I', 'will', 'have', 'my', 'vengeance', ',', 'in', 'this', 'life', 'or', 'the', 'next', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "raw = \"\"\"\n",
    "My name is Maximus Decimus Meridius, commander of the armies of the north, \n",
    "General of the Felix legions and loyal servant to the true emperor, Marcus Aurelius. \n",
    "Father to a murdered son, husband to a murdered wife. And I will have my vengeance, in this life or the next.\n",
    "\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "wordlemma = WordNetLemmatizer()\n",
    "lemmas = [wordlemma.lemmatize(t) for t in tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개체 이름 인식(NER, Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Seoul Botanical Garden is a well known place in Seoul, Korea.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Seoul/NNP)\n",
      "  (PERSON Botanical/NNP Garden/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  well/RB\n",
      "  known/VBN\n",
      "  place/NN\n",
      "  in/IN\n",
      "  (GPE Seoul/NNP)\n",
      "  ,/,\n",
      "  (GPE Korea/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Seoul Botanical Garden is a well known place in Seoul, Korea.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(words)\n",
    "chunks = nltk.ne_chunk(tags)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NE', [('Packt', 'NNP')]), Tree('NE', [('Birmingham', 'NNP')])]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)), binary=True)\n",
    "[a for a in i if len(a)==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 중의성 (disambiguity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n",
      "Synset('bank.v.07')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "sentence1 = \"Keep your savings in the bank\"\n",
    "print(lesk(word_tokenize(sentence1), 'bank'))\n",
    "\n",
    "sentence2 = \"It's so risky to drive over the banks of the road\"\n",
    "print(lesk(word_tokenize(sentence2), 'bank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
